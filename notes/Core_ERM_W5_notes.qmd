---
title: "Core_ERM_W5_notes"
format: html
---

# Week 5 Part 1: Running a Simulation Study

```{r}
options(crayon.enabled = FALSE) # suppress colorised warnings to be displayed correctly
gc() # garbage collection
rm(list = ls()) # clear variables
```

```{r}
library(tidyverse)
```

## More on `purrr`

-   References:

    -   [`purrr` cheatsheet](https://github.com/rstudio/cheatsheets/blob/main/purrr.pdf)

    -   [`purrr` tutorial](https://jennybc.github.io/purrr-tutorial/index.html)

    -   [R for Data Science, Chapter 21](https://r4ds.hadley.nz/iteration.html)

    -   [Advanced R Chapter 9](https://adv-r.hadley.nz/functionals.html)

    -   [Functional Programming](https://dcl-prog.stanford.edu/)

    ### Functional

    -   A **functional** is a function that:
        -   takes another function as input or
        -   returns another function as its output
    -   Example: `purrr::map(.x, .f)`:$$\text{map}(x,f) = \text{map}\left( \begin{bmatrix}
        x_{1} \\
        x_{2} \\
        \vdots \\
        x_{n}
        \end{bmatrix}, f \right) = \begin{bmatrix}
        f(x_{1}) \\
        f(x_{2}) \\
        \vdots \\
        f(x_{n})
        \end{bmatrix} $$
    -   We can use `map(.x, .f)` to replace `for()` loop

Example:

```{r}
#results <- vector("list", length(x)) # pre-allocate empty list
#for (i in seq_along(x)) { #safer than 1:length(x)
#  results[[i]] <- f(x[[i]]) # x is a list; items could be anything
#}
```

### `purrr::map()`

An example for `purrr:map()` :

```{r}
sum_prod <- function(v) {
  # return the sum and product of a vector v as named vector
  c("sum" = sum(v),
    "prod" = prod(v))
}

x <- list(c(1, 1, 1),
          c(1, 2, 3),
          c(3, 3, 3))

map(x, sum_prod)
```

If we input a vector, `purrr:map()` will treat it as a list:

```{r}
map(1:4, \(x) x^2)
```

### `purrr::map_dfc()` and `purrr:map_dfr()`

`map_dfc()` binds columns and returns a dataframe

Example:

```{r}
map_dfc(x, sum_prod) # no row names
```

`map_dfr()` binds rows and returns a dataframe:

```{r}
map_dfr(x, sum_prod) # column names are preserved
```

### `map_dbl(.x, .f)` and friends `map_xxx(.x,  .f)`

-   Common features:

    -   `.f` returns a scalar

    -   output an atomic vector with the same length as `.x`

-   `xxx` specifies the type of output:

    -   `map_dbl()` returns a double vector

    -   `map_chr()` returns a character vector

    -   `map_int()` returns an integer vector

    -   `map_lgl()` returns a logical vector

Examples:

```{r}
map_dbl(1:10, \(x) x^2)
```

```{r}
map_int(c(5L, 10L, 13L, 7L), \(x) x %% 3)
```

```{r}
map_chr(c("I", "Love", "Ruru"), tolower)
```

```{r}
map_lgl(c(TRUE, FALSE, TRUE), \(x) !x)
```

### Multiple Arguments: `purrr::map2(.x, .y, .f)`

$$\text{map2}\left( \begin{bmatrix}
x_{1} \\
x_{2} \\
\dots \\
x_{n}
\end{bmatrix}, \begin{bmatrix}
y_{1} \\
y_{2} \\
\dots \\
y_{n}
\end{bmatrix}, f \right) = \begin{bmatrix}
f(x_{1},y_{1}) \\
f(x_{2}, y_{2}) \\
\dots \\
f(x_{n}, y_{n})
\end{bmatrix} $$

-   `.x` and `.y` are vectors or lists of the same length

-   `.f` is a function that takes 2 arguments

-   returns a list by default; variants like `map2_dfr()`, `map2_dfc()`, `map2_dbl()` are analogous to `map()`

```{r}
map2_dbl(6:10, 1:5, \(x, y) x^y)
```

### Parallel Map: `pmap(.l, .f)`

$$\text{pmap}\left( \begin{bmatrix}
l_{11} & l_{12} &  \dots  & l_{1r} \\
\vdots \\
l_{n1} & l_{n2} & \dots & l_{nr}
\end{bmatrix}, f \right) = \begin{bmatrix}
f(l_{11}, l_{12}, \dots, l_{1r}) \\
\vdots \\
f(l_{n1}, l_{n2}, \dots, l_{nr})
\end{bmatrix}$$

-   `.l` is a list of lists or a list of vectors (dataframe)

-   `.f` is a function that takes `r` arguments

-   returns a list by default; variants exist

```{r}
df <- tibble(col1 = 1:5, col2 = 5:1, col3 = -2:2)

pmap_dbl(df, \(col1, col2, col3) col3 / (col1 + col2) ^ 2) # argument names have to match with df
```

## Simulations

## General Structure

Steps for running a simulation study:

1.  Write a function to generate simulation data

2.  Write a function to estimate parameters

3.  Run the simulation for fixed parameters. Repeat many times:

    1.  Draw simulation data

    2.  Calculate estimates

4.  Repeat step 3 over a grid range of parameter values.

5.  Store and summarize the results.

## Example of Simulation: Estimate Variance of Normal Distribution

```{r}
# function to generate sim data
draw_sim_data <- function(n, s_sq) {
  rnorm(n, sd = sqrt(s_sq))
}

# function to estimate parameters
get_estimates <- function(x) {
  c("usual" = var(x),
    "MLE" = mean((x - mean(x))^2))
}

# run sim for fixed parameters
set.seed(1145)
nreps <- 5000
sim_dataset <- map(1:nreps, \(i) draw_sim_data(n = 5, s_sq = 1))
sim_estimates <- map_dfr(sim_dataset, get_estimates)

# run sim over parameter grid
run_sim <- function(n, s_sq, nreps = 5000) {
  sim_datasets <- map(1:nreps, \(i) draw_sim_data(n, s_sq))
  map_dfr(sim_datasets, get_estimates)
}
set.seed(1693)
sim_params <- expand_grid(n = 3:5, s_sq = 1:3)
sim_results <- pmap(sim_params, run_sim)

# summarise results
get_summary_stats <- function(df) {
  df |>
    summarise(ususal_mean = mean(usual),
              MLE_mean = mean(MLE),
              usual_var = var(usual),
              MLE_var = var(MLE))
}

summary_stats <- sim_results |>
  map_dfr(get_summary_stats) |>
  bind_cols(sim_params)

summary_stats
```

### Design Choices in Simulation Studies

3 common approaches:

-   Generate all datasets first, then calculate estimates

-   Process one dataset at a time

-   Inline everything

#### Generate All Datasets First

```{r}
draw_sim_data <- function(n, s_sq) { rnorm(n, sd = sqrt(s_sq)) }
get_estimates <- function(data) { 
  c('usual' = var(data), 'MLE' = mean((data - mean(data))^2)) 
}

run_sim <- function(n, s_sq, nreps = 5000) {
  # Generate all datasets, then calculate estimates
  sim_datasets <- map(1:nreps, \(i) draw_sim_data(n, s_sq))
  map_dfr(sim_datasets, get_estimates)
}
```

-   Pros:

    -   clearest conceptual separation

    -   easier debugging

    -   can reuse datasets with different estimators

    -   enables parallelisation of separate steps

-   Cons: high memory usage

#### Process One Dataset at a Time

```{r}
run_single_rep <- function(n, s_sq) {
  data <- draw_sim_data(n, s_sq)  # Generate one dataset
  get_estimates(data)             # Calculate estimates immediately
}

run_sim_memory_efficient <- function(n, s_sq, nreps = 5000) {
  map_dfr(1:nreps, \(i) run_single_rep(n, s_sq))
}
```

-   Pros:

    -   low memory usage

    -   maintain function modularity

    -   good for large datasets

-   Cons:

    -   can't inspect intermediate data

    -   less flexibility for complex designs

#### Inline Everything

```{r}
run_sim_inline <- function(n, s_sq, nreps = 5000) {
  map_dfr(1:nreps, function(i) {
    # Generate data and calculate estimates inline
    data <- rnorm(n, sd = sqrt(s_sq))
    c('usual' = var(data), 'MLE' = mean((data - mean(data))^2))
  })
}
```

-   Pros:

    -   simplest code structure

    -   low memery usage

    -   fewer function calls

-   Cons:

    -   no resuable components

    -   harder to test parts separately

    -   less flexible

## Misc

### `Miscpossibly()`

`possibly()` is a function that handles errors gracefully, suppressing errors and storing a designated value in case of failure:

```{r}
x <- list("YYRR", 3, 4)
map_dbl(x, possibly(log, NA_real_))
```

### `tidyr::expand_grid()`

We can use `expand_grid()` to set up a parameter grid:

```{r}
sim_params <- expand_grid(n = 3:5, s_sq = 1:3)
sim_params
```

# Week 5 Part 2: Regression Discontinuity Design

Notes adapted from UCL ECON0021

-   RDD Basic Idea: $D_{i}$ is assigned based on whether some "running variable" $A_{i}$ is above/below a cutoff $c$

## Sharp RD

### Treatment Assignment

-   Treatment $D_{i}$ is a deterministic function of $A_{i}$:$$D_{i}=\begin{cases}
    1, &if\ A_{i} \geq c \\
    0, &if\ A_{i}<c
    \end{cases}$$
-   Extrapolation: we cannot perform matching on $A_{i}$ because there is *no common support*: only $Y_{1i}$ is observed if $A_{i}\geq c$ and only $Y_{0i}$ is observed if $A_{i}<c$. We need to *extrapolate* potential outcomes' dependence on $A_{i}$ outside of group

### Assumptions

-   **Continuity**: $\mathbb{E}[Y_{1}|A_{i}=a]$ and $\mathbb{E}[Y_{0}|A_{i}=a]$ are *continuous* functions of $a$ at cutoff $c$.
    -   This *requires*:
        -   $D_{i}$ jumps at $A_{i}=c$
        -   All factors determining $Y_{1i}, Y_{0i}$ change continuously around $c$
        -   **No manipulation** of assignment $A_{i}$ near the cutoff (individuals cannot precisely control $A_{i}$ near the cutoff) $\implies$ the distribution of $A_{i}$ should be continuous at $c$
    -   This *implies*:
        -   Variation in treatment $D_{i}$ near the cut-off is random (Conditional Random Assignment / Local RCT)
            -   Predetermined cahracteristics $X_{i}$ should have the same distribution just to the left/right of the cutoff
        -   We can use outcomes of individuals just to the left of $c$ as counter-factual for individuals just to the right of $c$
    -   Check by **McCrary Test**: check continuity of the running variable and other characteristics at the cutoff
        -   It is not a formal test for the continuity assumption: even if those characteristics are consistent with continuity, they do not imply continuity (there could be unobserved variables that affect $Y_{1i},Y_{0i}$ and jump at the cutoff)
-   **Binary Treatment**:$$D= \mathbb{1}\left\{ X\geq c \right\} $$
-   **Running Variable and Clear Cutoff**: $X$ is an observed running variable and $c$ is a *known* threshold. \### Sharp RD Estimand and Treatment Effect
-   **Sharp RD Estimand**:$$\beta_{SD}=\lim_{\epsilon \downarrow 0} \Big\{ \mathbb{E}[Y_{i}|A_{i}=c+\epsilon]-\mathbb{E}[Y_{i}|A_{i}=c-\epsilon] \Big\}$$
-   Under continuity assumption, Sharp RD estimand identifies TE as the cutoff $c$:$$\beta_{SD}=\mathbb{E}[Y_{1i}-Y_{0i}|A_{i}=c]$$
    -   If TE is homogenous, then $\beta_{SD}=ATE=ATT$

### Implementation

-   **Steps**:

1.  Specify 2 regression models for either side of the cutoff:$$\begin{cases}
    Y_{i}=\alpha_{i}+f_{l}(c-A_{i})+\epsilon_{li}, &if\ A_{i}<c \\
    Y_{i}=\alpha_{r}+f_{r}(A_{i}-c)+\epsilon_{ri}, &if\ A_{i}\geq c
    \end{cases}$$where $f_{r}(.),f_{l}(.)$ are some functions with $f_{r}(0)=0,f_{l}(0)=0$
2.  Estimate $\alpha_{l},\alpha_{r}$
3.  RD Estimand: $\beta_{RD}=\alpha_{r}-\alpha_{l}$

-   *Key choice*:
    -   Specification of $f_{r}(.),f_{l}(.)$
    -   Choice of **bandwidth**: how far away from the cutoff do we use data?
-   **Regression Specifications**
    -   **Ideal local randomised experiment**: $Y_{i}\perp A_{i}$ on either side of the cutoff:$$\begin{cases}
        Y_{i}=\alpha_{i}+\epsilon_{li}, &if\ A_{i}<c \\
        Y_{i}=\alpha_{r}+\epsilon_{ri}, &if\ A_{i}\geq c
        \end{cases}$$
    -   **Smoothly contaminated local randomised experiment**: $Y_{i}$ depends on $A_{i}$ at either side of the cutoff
        -   We then need to choose a linear slope, quadratic slope,..., or a Non-parametric function

## Fuzzy RD

### Setup

-   Treatment $D_{i}$ is still random given a value of $A_{i}$, but the probability of treatment given $A_{i}$ jumps at cutoff $c$:$$\lim_{\epsilon\downarrow 0} P(D_{i}=1|A_{i}=c+\epsilon )\neq \lim_{\epsilon\downarrow 0} P(D_{i}=1|A_{i}=c-\epsilon )$$
    -   Sharp RD is a special case of Fuzzy RD where the probability jumps from 0 to 1

#### Estimand

-   **Fuzzy RD Estimand**:$$\beta_{FD}=\frac{\lim_{\epsilon \downarrow 0} \Big\{ \mathbb{E}[Y_{i}|A_{i}=c+\epsilon]-\mathbb{E}[Y_{i}|A_{i}=c-\epsilon] \Big\}}{\lim_{\epsilon \downarrow 0} \Big\{ P[D_{i}=1|A_{i}=c+\epsilon]-P[D_{i}=1|A_{i}=c-\epsilon] \Big\}}$$

#### Fuzzy RD as IV

-   Fuzzy RD can be interpreted as IV near the cutoff
-   Instrument:$$Z_{i}=\begin{cases}
    1, &if\ A_{i}\geq c \\
    0, &if\ A_{i}<c
    \end{cases}$$
-   Potential treatment:$$D_{i}=\begin{cases}
    D_{1i}, &if\ Z_{i}=1 \\
    D_{0i}, &if\ Z_{i}=0
    \end{cases}$$
-   For small $\epsilon>0$, RD estimand can be understood as a Wald estimand, and identifies **LATE**:$$\begin{split}
    \beta_{FD}&\approx\frac{\mathbb{E}[Y_{i}|A_{i}=c+\epsilon]-\mathbb{E}[Y_{i}|A_{i}=c-\epsilon]}{P[D_{i}=1|A_{i}=c+\epsilon]-P[D_{i}=1|A_{i}=c-\epsilon]}\\
    &=\frac{\mathbb{E}[Y_{i}|Z_{i}=1]-\mathbb{E}[Y_{i}|Z_{i}=0]}{\mathbb{E}[D_{i}|Z_{i}=1]-\mathbb{E}[D_{i}|Z_{i}=0]}\\
    &=\mathbb{E}[Y_{1i}-Y_{0i}|D_{1i}>D_{0i}]
    \end{split}$$
    -   Interpretation as LATE:
        -   Local: ATE for people who are
            -   *At the cutoff* ($A_{i}=c$)
            -   *Switch to treatment at the cutoff (compliers)*
                -   Here, compliers are individuals who would not participate if $c$ is just above $A_{i}$, but would participate if $c$ is just below $A_{i}$

## Practical Advice

-   Motivate the validity of design -- why individuals cannot manipulate assignment variable $A_{i}$?
-   Test validity of design -- check continuity of outcomes, covariates, and density of $A_{i}$ around the cutoff
-   Show robustness of RD estimates with respect to specification of $f_{r}(.),f_{l}(.)$ and choice of bandwidth

# Week 5 Part 3: Research Plumbing II

```{r}
options(crayon.enabled = FALSE) # suppress colorised warnings to be displayed correctly
gc() # garbage collection
rm(list = ls()) # clear variables
```

## Merging Data

Sample data:

```{r}
library(tidyverse)
set.seed(92815)

gradebook <- tibble(
  student_id = c(192297, 291857, 500286, 449192, 372152, 627561), 
  name = c('Alice', 'Bob', 'Charlotte', 'Dante', 
           'Ethelburga', 'Felix'),
  quiz1 = round(rnorm(6, 65, 15)), quiz2 = round(rnorm(6, 88, 5)),
  quiz3 = round(rnorm(6, 75, 10)), midterm1 = round(rnorm(6, 75, 10)),
  midterm2 = round(rnorm(6, 80, 8)), final = round(rnorm(6, 78, 11)))

emails <- tibble(
  student_id = c(101198, 192297, 372152, 918276, 291857), 
  email = c('unclejoe@whitehouse.gov', 'alice.liddell@chch.ox.ac.uk',
            'ethelburga@lyminge.org', 'mzuckerberg@gmail.com',
            'microsoftbob@hotmail.com'))

```

`left_join()` as an example:

```{r}
left_join(gradebook, emails)
```

Others: `right_join()`, `full_join()`, `inner_join()`

## Reshaping Data

### `pivot_longer()`

```{r}
gradebook_pvt <- gradebook |>
  pivot_longer(
    starts_with("quiz"), # cols, using tidyselect here
    names_to = "quiz", # how to handle column names
    names_prefix = "quiz", # remove common prefix
    names_transform = list(quiz = as.numeric), # convert to numeric
    values_to = "score" # where to store value
  ) |>
  select(student_id, name, quiz, score)

gradebook_pvt
```

Making a plot:

```{r}
gradebook_pvt |>
  ggplot(aes(x = quiz, y = score)) +
  geom_line() + 
  geom_point() +
  facet_wrap(~ name) +
  theme_minimal()
```

### `pivot_wider()`

```{r}
gradebook_pvt |>
  pivot_wider(
    names_from = quiz, # extract name
    names_prefix = "quiz", # add prefix
    values_from = score # get corresponding values
  )
```

## Parallel Computing with `furrr`

### Basics

-   **Serial**: one step at a time; **Parallel**: many steps at once

-   A computing problem is called **embarrassingly parallel** if it is really easy to convert from serial to parallel.

    -   Typically: many independent tasks and little or no need to exchange information between the tasks

    -   Leading example: Monte Carlo Simulations

### 2 Common Approaches

-   **2 common approaches**:

    -   Fix parameters -\> spread simulation reps across cores -\> repeat

        -   Suitable only when each sim rep is low

    -   Spread parameters across cores -\> each core runs all steps

        -   Always better than serial

-   Doubling \# of cores generally *doesn’t* halve the run time:

    -   [Amdahl’s Law](https://en.wikipedia.org/wiki/Amdahl%27s_law): only part of your code is parallel.

    -   [Load Balancing](https://en.wikipedia.org/wiki/Load_balancing_(computing)): if a core has less work, it finishes early

    -   Communication overhead, hard/software limitations

### `future` and `furrr`

-   [`future`](https://future.futureverse.org/) provides *asynchronous* evaluation of R expressions:

    -   **multisession:** background R sessions on current machine

    -   **multicore:** forked R processes on current machine

    -   **cluster:** external R sessions on current/ local / remote machines

-   [`furrr`](https://furrr.futureverse.org/index.html) uses `future` to run `purrr` commands in parallel.

    -   Simply prefix `purrr` functions with `future_`

    -   E.g. `future_map()` or `future_pmap()`

### Example

```{r}
library(tictoc) # timing
library(furrr)
```

```{r}
# sim_params <- expand_grid(n = 3:12, s_sq = 1:10)
# plan(multisession, workers = 4)
# set.seed(1066)
# my_options <- furrr_options(seed = TRUE)
# tic()
#sim_results <- future_pmap(sim_params, run_sim, .options = my_options)
# toc()
```

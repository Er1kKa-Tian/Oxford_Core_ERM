---
title: "Core_ERM_W4_notes"
format: pdf
editor: visual
---

# Week 4 Part 1: The Multivariate Normal Distribution

```{r}
#.rs.restartR() #restart R session
```

```{r}
options(crayon.enabled = FALSE) # suppress colorised warnings to be displayed correctly
gc() # garbage collection
rm(list = ls()) # clear variables
```

## Generating Correlated Normal Draws

### Start with Independent Standard Normals

```{r}
# creat matrix z such that each row represents a draw of the random vector (Z1, Z2)
set.seed(1145)
n <- 1e5
z1 <- rnorm(n)
z2 <- rnorm(n)
z <- cbind(z1, z2)
rm(z1, z2)
head(z)
```

```{r}
# sample mean
colMeans(z)
```

```{r}
# var-cov matrix
var(z)
```

```{r}
# correlation matrix
cor(z)
```

### Visualising Marginal Distributions

-   Method 1: using histograms with simulated data

```{r}
library(tidyverse)
library(patchwork)

z1_hist <- as_tibble(z) |>
  ggplot(aes(x = z1)) +
  geom_histogram(fill = "red", alpha = 0.3)

z2_hist <- as_tibble(z) |>
  ggplot(aes(x = z2)) +
  geom_histogram(fill = "blue", alpha = 0.3)

z1_hist + z2_hist
```

-   Method 2: using KDE and simulated data

```{r}
z1_dens <- as_tibble(z) |>
  ggplot(aes(x = z1)) +
  geom_density(fill = "red", alpha = 0.3)

z2_dens <- as_tibble(z) |>
  ggplot(aes(x = z2)) +
  geom_density(fill = "blue", alpha = 0.3)

z1_dens + z2_dens
```

### Visualising Joint Distribution

```{r}
as_tibble(z) |>
  ggplot(aes(x = z1, y = z2)) +
  geom_density2d_filled() +
  coord_fixed()
```

### Change Means

```{r}
# shift means from (0, 0) to (1, -1)
x <- cbind(x1 = z[,1] + 1,
           x2 = z[,2] - 1)

x_marginals <- ggplot(as_tibble(x)) +
  geom_density(aes(x = x1),
               fill = "red",
               alpha = 0.3) +
  geom_density(aes(x = x2),
               fill = "blue",
               alpha = 0.3) +
  xlab("")

x_joint <- ggplot(as_tibble(x)) +
  geom_density2d_filled(aes(x = x1, y = x2)) +
  coord_fixed()

x_marginals + x_joint
```

### Change Variances

-   To change the variances of $Z_1$ and $Z_2$ without creating any covariance between them, multiply each by a constant:

```{r}
# change variance from (1, 1) to (4, 25)
x <- cbind(
  x1 = 2 * z[,1],
  x2 = 5 * z[,2]
)

cov(x)
```

```{r}
x_marginals <- ggplot(as_tibble(x)) +
  geom_density(aes(x = x1),
               fill = "red",
               alpha = 0.3) +
  geom_density(aes(x = x2),
               fill = "blue",
               alpha = 0.3) +
  xlab("")

x_joint <- ggplot(as_tibble(x)) +
  geom_density2d_filled(aes(x = x1, y = x2)) +
  coord_fixed()

x_marginals + x_joint
```

### Create Correlations by Combining Z1Z2

-   Construct $X_1$ and $X_2$ as linear combinations of $(Z_1, Z_2)$:

```{r}
x <- cbind(
  x1 = 2 * z[,1] + z[,2],
  x2 = z[,1] + 4 * z[,2]
)
```

```{r}
cov(x)
```

```{r}
cor(x)
```

-   Now the ellipses are tilted rather than axis-aligned:

```{r}
x_marginals <- ggplot(as_tibble(x)) +
  geom_density(aes(x = x1),
               fill = "red",
               alpha = 0.3) +
  geom_density(aes(x = x2),
               fill = "blue",
               alpha = 0.3) +
  xlab("")

x_joint <- ggplot(as_tibble(x)) +
  geom_density2d_filled(aes(x = x1, y = x2)) +
  coord_fixed()

x_marginals + x_joint
```

-   We can also generate the same sample using matrix multiplication:

```{r}
A <- matrix(
  c(2, 1,
    1, 4),
  2, 2,
  byrow = TRUE
)

x_alt <- t(A %*% t(z)) #t() is transposing; %*% is matrix multiplication

colnames(x_alt) <- c("x1", "x2")
identical(x, x_alt)
```

### Notations

-   Let $Z$ be a vector of $p$ iid standard normal RVs
-   Let $A$ be a $(p \times p)$ matrix of constants
-   Let $c$ be a $(p\times1)$ vector of constants
-   Then, $X = (c+AZ)$ is a multivariate normal RV

## The Cholesky Decomposition

-   Let $M$ be a symmetric and positive definite matrix, then:$$\exists A ~~\text{s.t.}~~ \Sigma =  A A^{T}$$
-   However, that $A$ is not unique. To make it unique, we can further focus on lower triangular matrices:$$\exists L~ \text{(lower triangular)} ~~\text{s.t.}~~ \Sigma = L L^{T}$$which is known as **Cholesky Decomposition**
-   Example:

```{r}
Sigma <- matrix(
  c(1, 0.5,
    0.5, 1),
  2, 2,
  byrow = TRUE
)

chol(Sigma)
```

# Week 4 Part 2: Instrumental Variables

## 2SLS in R

-   The `ivreg()` function from the `ivreg` package carries out 2SLS estimation with correct standard errors.

-   `tidy()`, `augment()`, `glance()` from `broom` work with `ivreg()`

-   `ivreg()` syntax: \`ivreg(\[CAUSAL_MODEL_FORMULA \| FIRST_STAGE_FORMULA\], data = \[DATAFRAME\])

    -   example: `ivreg(y  ~ x + w | z1 + z2 + w)`

-   Example

```{r}
options(crayon.enabled = FALSE) # suppress colorised warnings to be displayed correctly
gc() # garbage collection
rm(list = ls()) # clear variables
```

# Week 4 Part 3: Local Average Treatment Effects

## Wald Estimator

-   **Wald Estimator**: when treatment and IV are both binary$$\beta_{IV} = \frac{Cov(Z,Y)}{Cov(Z,D)} = \frac{\frac{Cov(Z,Y)}{Var(Z)}}{\frac{Cov(Z,D)}{Var(Z)}} = \frac{\mathbb{E}\left[ Y|Z=1  \right] - \mathbb{E}\left[ Y|Z=0 \right] }{\mathbb{E}\left[ D|Z=1 \right] -\mathbb{E}\left[ D|Z=0 \right] }$$

## ITT and LATE

### ITT

-   $\mathbb{E}\left[ Y|Z=1 \right]- \mathbb{E}\left[ Y|Z=0 \right]$ is the **Intention-to-Treatment Effect (ITT)** --- the causal effect of *offering* the treatment
-   Decomposing the ITT:
-   $\mathbb{E}\left[ Y|Z=1 \right]$ is a mixture of $Y_{0}$ and $Y_{1}$ for different types of individuals:$$\mathbb{E}\left[ Y|Z=1 \right] = (1-p_{1})\mathbb{E}\left[ Y_{0}|Z=1,D=0 \right] + p_{1}\mathbb{E}\left[ Y_{1}|Z=1,D=1 \right]  $$
-   Same for $\mathbb{E}\left[ Y|Z=0 \right]$:$$\mathbb{E}\left[ Y|Z=0 \right] = (1-p_{0})\mathbb{E}\left[ Y_{0}|Z=0,D=0 \right] + p_{0}\mathbb{E}\left[ Y_{1}|Z=0,D=1 \right] $$

### Types of Individuals

4 types of individuals $t \in \left\{ n, a, c,d \right\}$: - Always takers (n) $$D_{1i}=D_{0i}=1$$ - Never takers (a) $$D_{1i}=D_{0i}=0$$ - Compliers (c) $$D_{1i}=1, D_{0i}=0$$ - Defiers (d) $$D_{1i}=0, D_{0i}=1$$

### LATE

4 Assumptions (I used the more commonly used set of assumptions): - **Independence** (*random assignment of IV):* $$(Y_{11i},Y_{10i},Y_{01i},Y_{00i},D_{1i},D_{0i})\perp Z_i$$ - Implication: we can estimate causal effects of $Z_{i}$ on $D_{i}$ and $Z_{i}$ on $Y_{i}$ as in RCTs - **Exclusion** (IV does not directly influence potential outcomes*):* $$\begin{cases}
Y_{11i}=Y_{10i}=Y_{1i}\\Y_{01i}=Y_{00i}=Y_{0i}
\end{cases}$$ - Implication: we have the usual expression for observed outcomes: $$Y_{i}=D_{i}Y_{1i}+(1-D_{i})Y_{0i}=\underbrace{ \mathbb{E}[Y_{0i}] }_{ \alpha }+(\underbrace{ Y_{1i}-Y_{0i} }_{ \beta })D_{i}+(\underbrace{ Y_{0i}-\mathbb{E}[Y_{0i}] }_{ u_{i} })$$ - **Monotonocity** (No defiers*): increasing IV may induce some untreated individuals to take treatment, but cannot induce any treated individuals to leave treatment:*$$D_{1i}\geq D_{0i}\ \forall\ i$$ - **Relevance** (IV induces variation in the treatment\*): IV needs to induce variations in treatment indicator:$$\mathbb{E}[D_{1i}-D_{0i}]\neq 0$$ - This is satisfied if individuals with $Z_{i}=1$ are more likely to participate:$$P(D_{1i}=1)=\mathbb{E}[D_{1i}]>\mathbb{E}[D_{0i}]=P(D_{0i}=1)$$

Under those 4 assumptions, Wald estimator identifies **Local Average Treatment Effect** (ATE on compliers):$$LATE \equiv \mathbb{E}[Y_{1i}-Y_{0i}|D_{1i}>D_{0i}]$$is the average treatment effect for those who are induced to take the treatment when the IV is increased (**treatment effect for the compliers only**) - *Under independence + exclusion + monotonicity + relevance, Wald Estimand identifies LATE*:$$\beta_{Wald}=LATE\ \begin{cases}=ATE=ATT & \text{if Homogeous TE}\\\neq ATE\ or\ ATT & \text{if Heterogenous TE}  \end{cases} $$ - LATE is a **local effect** (only in complier group) - different instruments identify different LATE (we need policy relevant instrument)

36:44

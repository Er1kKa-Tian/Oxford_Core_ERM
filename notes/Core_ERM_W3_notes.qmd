---
title: "Core_ERM_W3_notes"
author: "Er1kKa"
format: pdf
editor: visual
header-includes:
  - \usepackage{mhchem}
---

```{r}
options(crayon.enabled = FALSE) # suppress colorised warnings to be displayed correctly
gc() # garbage collection
rm(list = ls()) # clear variables
```

# Week 3 Part 1: Logistic Regression

## Logistic Regression Simulation

```{r}
library(tidyverse)
set.seed(1234)
n <- 500
x <- rnorm(n, mean = 1.5, sd = 2) # generate x
ystar <- 0.5 + 1 * x + rlogis(n) # generate latent variable
y <- 1 * (ystar > 0) # threshold corssing transformation
mydat <- tibble(x, y)
mydat
```

## Generalised Linear Models

### Intro

`glm(formula, family, data)`

-   this is a base R function to estimate generalised linear models

-   formula and data work just like `lm()`

-   family describe the error distribution and link function

    -   `family = binomial(link = "logit)`

    -   `family = binomial(link = "probit")`

    -   `family = poisson(link = "log")`

-   Compatible with `tidy(), glance(), augment()`

### Logit

```{r}
lreg <- glm(y ~ x, family = binomial(link = "logit"), mydat)
summary(lreg)
```

### Predicted Probabilities

-   `predict()` works with `glm()` almost as it does for `lm()`

-   To get predicted probabilities, set `type = "response"`

Example:

```{r}
# P(Y = 1 | X = 0)
predict(lreg, newdata = data.frame(x = 0), type = "response")
```

```{r}
# P (Y = 1 | X = mean(X))
predict(lreg, newdata = data.frame(x = mean(x)), type = "response")
```

```{r}
# P(Y = 1 | X = observed values)
p_hat <- predict(lreg, type = "response")
head(p_hat)
```

`augment()` with `glm()` objects:

```{r}
library(broom)
augment(lreg, mydat, type.predict = "response")
```

### Plotting Logit Regressions

```{r}
ggplot(mydat, aes(x, y)) +
  stat_smooth(method = "glm", method.args = list(family = "binomial")) +
  geom_point()
```

-   Notice the new argument to `stat_smooth()`

Use jittering to improve legibility:

```{r}
ggplot(mydat, aes(x, y)) +
  stat_smooth(method = "glm", method.args = list(family = "binomial")) +
  geom_jitter(width = 0.5, #add noise to x-coordinates,
              height = 0.1 #add noise to y-coordinates
              )
```

# Week 3 Part 2: Selection on Observables

## Potential Outcomes Framework

-   Binary **treatment**:$$D \in \left\{ 0, 1 \right\} $$
-   **Observed outcome** $Y$ depends on **potential outcomes** $(Y_{0}, Y_{1})$ and can be denoted:$$\begin{align}
    Y &  = (1-D) Y_{0} + DY_{1} \\
     & = Y_{0} + D(Y_{1} - Y_{0})
    \end{align}$$

## Treatment Effects and Fundamental Fundamental Problem of Causal Inference

-   **Average Treatment Effect (ATE)**:$$ATE \equiv \mathbb{E}\left[ Y_{1} - Y_{0} \right] $$

-   **Average Treatment Effect on the Treated (TOT/ATT)**:$$TOT\equiv ATT\equiv \mathbb{E}\left[ Y_{1} - Y_{0}|D=1 \right] $$

-   **Conditional Average Treatment Effect (CATE)**:$$CATE \equiv \mathbb{E}\left[ Y_{1}-Y_{0}|X=x \right] $$

-   **Fundamental Problem of Causal Inference**: we can never observe both $Y_{0}$ and $Y_{1}$ at the same for the same person $\implies$ we cannot learn the joint distribution of the potential outcomes

## Naive Comparison of Means and Selection Bias

-   It can be shown that:$$\underbrace{ \mathbb{E}\left[ Y|D=1 \right] - \mathbb{E}\left[ Y|D=0 \right] }_{ \text{Observed Difference} } = \underbrace{ \mathbb{E}\left[ Y_{1}-Y_{0}|D=1 \right] }_{ ATT/TOT } + \underbrace{ \mathbb{E}\left[ Y_{0}|D=1 \right] - \mathbb{E}\left[ Y_{0}|D=0 \right] }_{ \text{Selection Bias} }  $$
-   Issues:
    -   Selection bias $\neq 0 \implies$ Observed difference $\neq$ ATT/TOT
    -   Selection on gains $\implies$ ATT/TOT $\neq$ ATE

## Randomisation

Random assignment of treatment ensures:$$D \statindep (Y_{0},Y_{1})$$ This implies: - No selection bias - $TOT = ATE$ $\implies$ Observed difference = ATE

However, in reality, it's often impossible to have random assignment.

## Retrieve Causal Effects without Randomisation

### 2 Key Assumptions

-   **Selection on Observables**: conditioning on observed characteristics $X$, treatment is independent of potential outcomes:$$D \statindep (Y_{0},Y_{1})|X$$

-   **Overlap/Common Support**: there are treated and untreated samples for all possible values of $X$:$$0< P(X) < 1$$

Whether those assumptions are plausible?

-   Selection on observables cannot be tested without additional assumptions or data.

-   Common support can be checked.

If add more characteristics to $X$, then it's more likely to satisfy Selection on Observables but less likely to satisfy Common Support.

-   Caveat: adding more controls is not always better --- beware of the Bad Controls! (W3P3)

### Method 1: Regression Adjustment

**Basic Idea**

1.  Compute CATE for each possible $X$:$$CATE(x) = \mathbb{E}\left[ Y|D=1,X=x \right] - \mathbb{E}\left[ Y|D=0, X=x \right]  $$
2.  Get ATE using LIE:$$ATE = \int_{x} CATE(x) \, dF(x) $$

**Implementation** Assume all covariates are binary. 1. Center all covariates $X$ around their means:$$\tilde{X} \equiv X - \overline{X}$$ 2. Regress $Y$ on $D, \tilde{X}$ and all interactions 3. The coefficient on $D$ the ATE with correct SE

### Method 2: Propensity Score Re-weighting

Idea: upweight under-represented groups

**Propensity Score Re-weighting**:$$ATE = \mathbb{E}\left[ \underbrace{ \frac{D}{P(X)} }_{ w_{1} } \cdot Y \right] - \mathbb{E}\left[ \underbrace{ \frac{1-D}{1-P(X)} }_{ w_{0} }\cdot Y \right] $$

## An Example

```{r}
.rs.restartR() #restrat R session
```

```{r}
options(crayon.enabled = FALSE) # suppress colorised warnings to be displayed correctly
gc() # garbage collection
rm(list = ls()) # clear variables
```

```{r}
library(tidyverse)

#generate example data
people <- c("Aiden", "Bella", "Carter", "Dakota", "Ethel", "Floyd",
"Gladys", "Herbert", "Irma", "Julius")
x <- c("young", "young", "young", "young", "old", "old",
"old", "old", "old", "old")
y0 <- c(1, 1, 1, 1, 0, 0, 0, 0, 0, 0)
y1 <- c(1, 1, 1, 1, 1, 0, 0, 1, 0, 0)
d <- c(0, 0, 0, 1, 0, 0, 0, 1, 1, 1)
y <- (1- d) * y0 + d * y1

tbl <- tibble(name = people, d, y, y0, y1, x)
rm(y0, y1, d, y, x, people)

tbl
```

```{r}
ATE <- tbl |>
  summarise(mean(y1 - y0)) |>
  pull()

ATE
```

```{r}
TOT <- tbl |>
  filter(d == 1) |>
  summarise(mean(y1 - y0)) |>
  pull()

TOT
```

```{r}
selection_bias <- tbl |>
  group_by(d) |>
  summarise(y0_mean = mean(y0)) |>
  pull(y0_mean) |>
  diff()

selection_bias
```

```{r}
means <- tbl |>
  group_by(d) |>
  summarise(y_mean = mean(y))

means
```

```{r}
naive_diff_in_means <- means |>
  pull(y_mean) |>
  diff()

naive_diff_in_means
```

```{r}
# CATE
tbl |>
  group_by(x) |>
  summarise(CATE = mean(y1 - y0))
```

```{r}
# ATE by LIE

group_stats <- tbl |>
  group_by(x) |>
  summarise(CATE_x = mean(y1 - y0), count = n()) |>
  mutate(p_x = count / sum(count))

group_stats |>
  summarise(sum(CATE_x * p_x)) |>
  pull()
```

```{r}
# Regression adjustment
library(broom)
tbl |>
  mutate(old = (x == "old"), xtilde = old - mean(old)) |>
  lm(y ~ d *xtilde, data = _) |>
  tidy() |>
  filter(term == "d")
```

```{r}
# Propensity score re-weighting

psw <- tbl |>
  group_by(x) |>
  mutate(pscore = mean(d)) |>
  ungroup() |>
  mutate(
    weight1 = d / pscore,
    weight0 = (1 - d) / (1 - pscore)
  )

psw |>
  summarise(mean(weight1 * y) - mean(weight0 * y)) |>
  pull()
```

## Further Discussions

### ATE or TOT?

Do we want ATE?

-   TOT/ATT is the average effect of a treatment for people who *voluntarily* take it.

Can we get ATE?

-   We often have selection on observables if we assume rational choice.

### Identifying TOT with Weaker Assumptions

Weaker assumptions: - Treatment provides no information on $Y_{0}$ given $X$:$$\mathbb{E}\left[ Y_{0}|D,X \right]  = \mathbb{E}\left[ Y_{0}|X \right] $$ - Existence of not-treated samples:$$Pr(x) <1 \ \forall\ x \in \text{support}(X)$$

Under those 2 assumptions:$$TOT/ATT = \mathbb{E}\left[ Y|D=1 \right] - \mathbb{E}_{X|D=1}\left[ Y|D=0,X \right] $$

# Week 3 Part 3: DAGs and Bad Controls

## "No causes in; no causes out."

-   We cannot simply control for any observed variable that is correlated with both treatment and outcome.

-   Meaningful causal inference always requires assumptions, even in RCTs.

-   Causal inference from observational data requires even more assumptions.

-   However, if you make your assumptions explicit, there is a definite answer:

    -   If it's possible to use selection-on-observables, then we can find the correct controls.

    -   If it's not possible, we can show why.

## Graphs and DAGs

### Basic Concepts

-   Concepts in a graph

    -   **Graph**: set of nodes connected by edges.

    -   Two nodes are **adjacent** if connected by an edge.

    -   Directed edge points from **parent** to **child**.

    -   **Directed graph** has only directed edges.

    -   **Path**: sequence of connected vertices.

    -   **Directed path**: a path that "obeys one-way signs".

    -   Directed path points from **ancestor** to **descendant**.

    -   **Cycle**: directed path that returns to starting node.

    -   **Acyclic cycle**: a graph without any cycles.

-   Concepts in Directed Acyclic Graphs (**DAG**s)

    -   If $D$ is ancestor of $Y$, it is a **potential cause** of $Y$.

    -   If $D$ is a parent of $Y$, it is a **direct cause** of $Y$.

### 4 Common Structures

```{r}
.rs.restartR() #restar R session
```

```{r}
options(crayon.enabled = FALSE) # suppress colorised warnings to be displayed correctly
gc() # garbage collection
rm(list = ls()) # clear variables
```

```{r}
library(tidyverse)
library(ggdag)
```

#### Fork/Common Cause/Confounder

```{r}
dag_fork <- dagify(Y ~ X, D ~ X)

ggdag(dag_fork) +
  theme_dag()
```

-   Confounders are good controls!

    -   $D$ and $Y$ are dependent because there is an open path between them.

    -   However, $D$ does cause $Y$.

    -   Therefore, we need to condition on $X$ to block the path from $D$ to $Y$.

-   **Fork Rule**: if $X$ is a common cause of $D$ and $Y$, and there is only one path between $D$ and $Y$, then:$$D \statindep Y|X$$

#### Pipe/Mediator

```{r}
dag_pipe <- dagify(Y ~ X, X ~ D)

ggdag(dag_pipe) +
  theme_dag()
```

-   Mediators are bad controls!
    -   $D$ and $Y$ are dependent because there is an open path between them.
    -   $D$ causes $Y$ through $X$
    -   Conditioning on $X$ blocks the path from $D$ to $Y$
-   **Pipe Rule**: if there is only one directed path from $D$ to $Y$ and $X$ intercepts the path, then:$$D\statindep Y|X$$

#### Collider/Common Effect

```{r}
dag_collider <- dagify(X ~ D, X ~ Y)

ggdag(dag_collider) +
  theme_dag()
```

-   Common effects are bad controls!
    -   $D$ and $Y$ are independent because the path between them is blocked.
    -   $D$ and $Y$ both cause $X$, but neither causes the other.
    -   Conditioning on $X$ *unblocks* the path between $D$ and $Y$
-   **Collider Rule**: if there is only one path between $D$ and $Y$, and $X$ is their common effect, then:$$D\statindep Y ~ ~ \text{but} ~ ~ D \cancel\statindep Y |X$$

#### Descendant

```{r}
# this is only an examle of descendant
dag_descandant <- dagify(X ~ D, X ~ Y, Z ~ X)

ggdag(dag_descandant) +
  theme_dag()
```

-   Descendants are "addons" to a node. It can be added to all 3 scenarios above.
-   Descendant has the same property as its parent.
-   **Descendant Rule**: conditioning on a descendant $Z$ of $X$ has the same effect of partially conditioning on $X$ itself.

## Graph Surgery

### Causal and Non-causal Paths

-   **Causal/Frontdoor Path**: directed path between the treatment and outcome; always starts with an edge pointing *out* of treatment.

-   **Non-causal/Backdoor Path**: path between treatment and outcome; always starts with an edge pointing *into* treatment.

### Observational and Interventional Distribution

-   **Observational Distribution** $Pr(Y|D=d)$

    -   This is the *actual* distribution of $Y$ among people observed to have $D=d$

    -   DAG illustrates the observational distribution and how it arises from our causal model.

-   **Interventional Distribution** $Pr(Y|do(D=d))$

    -   This is the distribution of $Y$ we *would obtain* if we *intervened* and set $D=d$ for everyone.

    -   We can obtain it from *removing edges pointing into* $D$ (i.e. removing all backdoor paths)

    -   Causal effect of interest is the path from $D$ to $Y$ in this modified graph:$$ATE  = \mathbb{E}\left[ Y_{1} - Y_{0} \right] = \mathbb{E}\left[ Y|do(D=1) \right]  - \mathbb{E}\left[ Y|do(D=0) \right] $$

    -   This is what an experiment does — removing all causes of treatment.

## Backdoor Criterion

### Backdoor Criterion

-   Formal statement: a set of nodes $X$ satisfies the backdoor criterion relative to $(D,Y)$ if no node in $X$ is a descendant of $D$ and $X$ blocks every backdoor path between $D$ and $Y$.

-   A more intuitive explanation:

1.  list all the paths that cnonnect treatment and outcome
2.  check which of them open (a path is open unless it contains a collider)
3.  check which of them are backdoor paths (contain an arrow pointing at $D$)
4.  if there are no open backdoor paths, then you are done. If not, look for nodes you can condition on to block remianing open backdoor paths without opening new ones.

-   Note that: in a given DAG, there may ne multiple ways or no way to satisfy the backdoor criterion, given what we observe.

### Equivalent to Selection on Observables

-   If $X$ satisfies the backdoor criterion relative to $(D,Y)$, then:$$Pr(Y=y|do(D=d)) = \sum_{\text{all }x}Pr(Y=y|D=d,X=x)\cdot P(x=x)$$and we have the following counterfactual interpretation:$$Y_{d}\statindep D|X\ \forall\ d$$
-   In other words, backdoor criterion implies selection on observables assumption for $D$ given $X$, and the formula above is essentially the regression adjustment.
